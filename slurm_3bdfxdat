#!/bin/bash
#SBATCH -N 1

export SLURM_TASKS_PER_NODE=$SLURM_CPUS_ON_NODE

# ========================================
# Temporary working directory, see https://gooseslurm.readthedocs.io/en/latest/examples/tempdir/readme.html#job-script
# ========================================


# I. Define directory names [DO NOT CHANGE]
# =========================================

# get name of the temporary directory working directory, physically on the compute-node
workdir="${TMPDIR}"

# get submit directory
# (every file/folder below this directory is copied to the compute node)
submitdir="${SLURM_SUBMIT_DIR}"

# 1. Transfer to node [DO NOT CHANGE]
# ===================================

# create/empty the temporary directory on the compute node
if [ ! -d "${workdir}" ]; then
  mkdir -p "${workdir}"
else
  rm -rf "${workdir}"/*
fi

# change current directory to the location of the sbatch command
# ("submitdir" is somewhere in the home directory on the head node)
cd "${submitdir}"
# copy all files/folders in "submitdir" to "workdir"
# ("workdir" == temporary directory on the compute node)
cp -prf pdf* XDATCAR* POSCAR ${workdir}
# change directory to the temporary directory on the compute-node
cd ${workdir}

# 3. Function to transfer back to the head node [DO NOT CHANGE]
# =============================================================

# define clean-up function
function clean_up {
  # - remove log-file on the compute-node, to avoid the one created by SLURM is overwritten
  rm job.slurm.out
  # - delete temporary files from the compute-node, before copying
  # rm -r ...
  # - change directory to the location of the sbatch command (on the head node)
  cd "${submitdir}"
  # - copy everything from the temporary directory on the compute-node
  cp -prf "${workdir}"/3bdf* .
  # - erase the temporary directory from the compute-node
  rm -rf "${workdir}"/*
  rm -rf "${workdir}"
  # - exit the script
  exit
}

# call "clean_up" function when this script exits, it is run even if SLURM cancels the job
trap 'clean_up' EXIT

# 2. Execute [MODIFY COMPLETELY TO YOUR NEEDS]
# ============================================

# -----------------------------------------------
# define your histogram
rmin=0.7
rmax=6.1
dr=0.03
# -----------------------------------------------

# -----------------------------------------------
# divided "nsample" XDATCAR into "njobs" jobs each has "nxdat" files
nsamples=800
njobs=40
nxdat=20
# -----------------------------------------------

date > RunAt
hostname >> RunAt
echo running in `pwd` >> RunAt

init_bins.sh $rmin $rmax $dr
cat XDATCAR.lammps | split_xdat.awk

status=3bdfxdat.status
rm $status;
touch $status;

for i in `seq 0 1 $njobs`;
do
    if [ $i -eq $njobs ];
    then
	break;
    fi
    
    nstart=$(( $i*$nxdat+1 ))
    nend=$(( ($i+1)*$nxdat ))
    echo $nstart $nend >> 3bdfxdat_bundle.sh.out
    3bdfxdat_bundle.sh $nstart $nend $nstart >> $status &
done

while true;
do
    sleep 3;
    ans=`grep DONE $status | wc -l`;
    if [ $ans -eq $njobs ];
    then
	break;
    fi
done

echo Done `date` >> RunAt

sleep 10
